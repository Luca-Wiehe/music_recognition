{"cells":[{"cell_type":"markdown","metadata":{"id":"MI67emSTOyRm"},"source":["## Connect to Google Colab"]},{"cell_type":"markdown","metadata":{"id":"wbrUMjD_O2b_"},"source":["To train the network on a GPU, Google Colab will be used. To connect to Google Colab successfully, adjust `gdrive_path` so that it represents the root folder of the repository. In case you don't want to use Google Colab you can skip this cell."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22205,"status":"ok","timestamp":1706565812271,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"},"user_tz":-60},"id":"c8lXIGhqTOdV","outputId":"e601166b-c022-4a72-a919-cd8e00336ef2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","['.git', '.gitignore', 'README.md', 'data', 'data_loader.ipynb', 'networks', 'utils']\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# set path to project folder\n","gdrive_path='/content/gdrive/MyDrive/7-programming/music_recognition/'\n","\n","# mount Google Drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# navigate to Google Drive folder\n","os.chdir(gdrive_path)\n","\n","# check that we are in the right folder\n","print(sorted(os.listdir()))"]},{"cell_type":"markdown","metadata":{"id":"3wqFniGfPziU"},"source":["## Manage Imports"]},{"cell_type":"markdown","metadata":{"id":"IqY3wxHvP7Ra"},"source":["Throughout the repository, we will use a wide range of external packages. If you use Google Colab, these packages need to be installed using `pip` every time you restart the kernel. If you don't use Google Colab, you can create a virtual environment to manage packages and versions. In this case, you only need to install these packages with `pip` once."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8551,"status":"ok","timestamp":1706565832229,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"},"user_tz":-60},"id":"0325N3hoQldM"},"outputs":[],"source":["import torch\n","from data.primus_dataset import PrimusDataset, split_data, visualize_sample\n","\n","# set up default cuda device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"Xc7txo1hR6jd"},"source":["## Load Data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dccWQ1soSGfW"},"source":["Next, we will load the Primus dataset that will be used throughout this notebook. The dataset was published as part of the following publication:\n","\n","```\n","@inproceedings{calvo2018camera,\n","  title={Camera-PrIMuS: Neural End-to-End Optical Music Recognition on Realistic Monophonic Scores.},\n","  author={Calvo-Zaragoza, Jorge and Rizo, David},\n","  booktitle={ISMIR},\n","  pages={248--255},\n","  year={2018}\n","}\n","```\n","\n","It contains 87,678 labeled images of monophonic scores and can be downloaded from https://grfia.dlsi.ua.es/primus/. Make sure to specify the `data_root`-path to the dataset correctly. In our case, we stored it in `/<repository_root>/data/primus/`. Each sample in that dataset consists of a directory that is named e.g. `000051650-1_1_1` and contains 6 files.\n","*   `000051650-1_1_1_distorted.jpg`: The sample image with applied transformations\n","*   `000051650-1_1_1.agnostic`: Graphical elements with their position in the score\n","*   `000051650-1_1_1.mei`: Semantically labeled image in `.mei`-format, i.e. XML representation\n","*   `000051650-1_1_1.png`: The original image without any transformations\n","*   `000051650-1_1_1.semantic`: Semantically labeled image\n","*   `regular_pae.pae`: Image in format of data source taken from RISM-dataset\n","\n","Note that we will make predictions based on the undistorted `.png`-image and the `.semantic`-labeling of music notation.\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1200,"status":"ok","timestamp":1706565839139,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"},"user_tz":-60},"id":"Wo3o5HHcTdwY","outputId":"03a85e95-8638-44d2-8292-51d8eb736b36"},"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+-----------+\n","| Dataset    | # Samples |\n","+------------+-----------+\n","| Train      |     6     |\n","| Validation |     2     |\n","| Test       |     2     |\n","+------------+-----------+\n"]}],"source":["# load dataset\n","data_path = os.path.join(gdrive_path, 'data', 'primus')\n","vocabulary_path = os.path.join(gdrive_path, 'data', 'semantic_labels.txt')\n","dataset = PrimusDataset(data_path=data_path, vocabulary_path=vocabulary_path)\n","\n","# apply train-val-test split\n","train_data, val_data, test_data = split_data(dataset)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":1164,"status":"ok","timestamp":1706565842667,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"},"user_tz":-60},"id":"EdxMv8OCUB2I","outputId":"af22da56-1286-4344-daa2-6889a2a70b33"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["labels.shape: torch.Size([21])\n"]}],"source":["visualize_sample(train_data)"]},{"cell_type":"markdown","source":["## Train Model"],"metadata":{"id":"iR5oynqTd6Km"}},{"cell_type":"markdown","source":["Having loaded the dataset, we can start defining our model. For this, we need to define some hyperparameters that will be necessary for training. Use the following cell to define your own training parameters."],"metadata":{"id":"Gic20FI8fI3A"}},{"cell_type":"code","source":["hparams = {\n","    \"batch_size\": 3,\n","    \"epochs\": 10,\n","    \"early_stop\": {\n","        \"patience\": 25\n","    },\n","    \"scheduler\": {\n","        \"plateau_patience\": 10,\n","        \"plateau_decay\": 0.4,\n","        \"threshold\": 0.00005,\n","        \"threshold_mode\": \"rel\",\n","        \"mode\": \"min\",\n","        \"cooldown\": 0,\n","        \"eps\": 1e-8\n","    },\n","    \"optimizer\": {\n","        \"learning_rate\": 1e-3,\n","        \"weight_decay\": 1e-4\n","    },\n","}"],"metadata":{"id":"i5ZHUQKfddjS","executionInfo":{"status":"ok","timestamp":1706565845217,"user_tz":-60,"elapsed":202,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Given these model parameters, we can initialize our model. For the Primus dataset we use the `MonophonicModel` that is defined in `monophonic_nn.py`. It is suitable to predict sequences of unknown length (since we don't know in advance how many music symbols are depicted in an image). Note that we primarily focus on monophonic scores, i.e. a single line of music notation."],"metadata":{"id":"FNzgox-we5q9"}},{"cell_type":"code","source":["from networks.monophonic_nn import MonophonicModel\n","from utils.utils import create_tqdm_bar\n","\n","model = MonophonicModel(\n","    hparams=hparams,\n","    output_size=len(dataset.index_to_vocabulary)\n",")"],"metadata":{"id":"Et2PrR-FdzLw","executionInfo":{"status":"ok","timestamp":1706565848867,"user_tz":-60,"elapsed":1327,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Next, we run the training epochs of our model. After each epoch, we run our model on validation data to track the model's performance over time. We use early stopping which means that we terminate training early once the performance gain doesn't improve for several epochs."],"metadata":{"id":"1B4ilDCQiRBe"}},{"cell_type":"code","source":["def collate_fn(batch):\n","    \"\"\" Custom collate function for DataLoader. \"\"\"\n","    # Separate data and labels\n","    data, labels = zip(*batch)\n","\n","    # Find the maximum width and height in the batch\n","    max_width = max([d.shape[2] for d in data])\n","    max_height = max([d.shape[1] for d in data])\n","\n","    # Handling data (images)\n","    padded_data = []\n","    for d in data:\n","        # Calculate padding size\n","        padding_left = (max_width - d.shape[2]) // 2\n","        padding_right = max_width - d.shape[2] - padding_left\n","        padding_top = (max_height - d.shape[1]) // 2\n","        padding_bottom = max_height - d.shape[1] - padding_top\n","\n","        # Apply padding\n","        padded = torch.nn.functional.pad(d, (padding_left, padding_right, padding_top, padding_bottom), \"constant\", 0)\n","        padded_data.append(padded)\n","\n","    # Stack all the padded images and labels into tensors\n","    padded_data = torch.stack(padded_data)\n","\n","    # Handling labels\n","    # Find the maximum label length\n","    max_label_len = max([len(l) for l in labels])\n","\n","    # Pad labels\n","    padded_labels = []\n","    for l in labels:\n","        # Padding length\n","        padding_len = max_label_len - len(l)\n","\n","        # Pad and append\n","        padded_label = torch.cat((l, torch.full((padding_len,), -1, dtype=torch.long))) # Using -1 as padding token\n","        padded_labels.append(padded_label)\n","\n","    # Stack padded labels\n","    labels = torch.stack(padded_labels)\n","\n","    return padded_data, labels"],"metadata":{"id":"ZFi5hrjoJyjj","executionInfo":{"status":"ok","timestamp":1706565852532,"user_tz":-60,"elapsed":225,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def train_model(model, loss_func=torch.nn.CTCLoss(), epochs=20):\n","\n","      # obtain model optimizer\n","      optimizer = model.optimizer\n","\n","      # decrease lr of optimizer when reaching a plateau\n","      scheduler_hparams = hparams[\"scheduler\"]\n","      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","                  optimizer=optimizer,\n","                  mode=scheduler_hparams[\"mode\"],\n","                  patience=scheduler_hparams[\"plateau_patience\"],\n","                  factor=scheduler_hparams[\"plateau_decay\"],\n","                  threshold=scheduler_hparams[\"threshold\"],\n","                  threshold_mode=scheduler_hparams[\"threshold_mode\"],\n","                  cooldown=scheduler_hparams[\"cooldown\"],\n","                  eps=scheduler_hparams[\"eps\"],\n","      )\n","\n","      # select device for model\n","      model = model.to(device)\n","\n","      # initialize early stopping criteria\n","      stopping_hparams = hparams[\"early_stop\"]\n","      best_loss, best_model, best_optimizer = -1, None, None\n","      patience, current_patience = stopping_hparams[\"patience\"], stopping_hparams[\"patience\"]\n","\n","      # run epochs\n","      for epoch in range(epochs):\n","\n","          # training for each minibatch\n","          train_loader = torch.utils.data.DataLoader(train_data, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n","          train_loop = create_tqdm_bar(train_loader, desc=f\"Training Epoch [{epoch + 1}/{epochs}]\")\n","          train_loss, val_loss = 0, 0\n","\n","          for train_iteration, batch in train_loop:\n","\n","              # perform training step\n","              print(\"\\nAbout to perform training iteration\")\n","              loss = model.training_step(batch, loss_func, device)\n","              print(f\"Training iteration completed! Loss: loss.item()\")\n","              train_loss += loss.item()\n","\n","              # Update the progress bar.\n","              train_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(\n","                  train_loss / (train_iteration + 1)), val_loss = \"{:.8f}\".format(val_loss)\n","              )\n","\n","          # validation for each minibatch\n","          val_loader = torch.utils.data.DataLoader(val_data, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n","          val_loop = create_tqdm_bar(val_loader, f\"Validation Epoch [{epoch + 1}/{epochs}]\")\n","          val_loss = 0\n","\n","          for val_iteration, batch in val_loop:\n","\n","              # perform validation step\n","              loss = model.validation_step(batch, loss_func, device)\n","              val_loss += loss.item()\n","\n","              # update the progress bar.\n","              val_loop.set_postfix(val_loss = \"{:.8f}\".format(val_loss / (val_iteration + 1)))\n","\n","          # learning rate update for each epoch\n","          pre_lr = optimizer.param_groups[0][\"lr\"]\n","          scheduler.step(val_loss)\n","          post_lr = optimizer.param_groups[0]['lr']\n","          if post_lr < pre_lr:\n","            print(\"Loading best model due to learning rate decrease.\")\n","            model = best_model.to(device)\n","\n","          # check for early stopping\n","          if val_loss < best_loss or best_loss == -1:\n","              current_patience = patience\n","              best_loss = val_loss\n","              best_model = model\n","              best_optimizer = optimizer.state_dict()\n","          else:\n","              current_patience -= 1\n","\n","              if current_patience == 0:\n","                  print(f\"\\n{'===' * 10}\\nStopping early at epoch {epoch}\\n{'===' * 10}\")\n","                  model.load_state_dict(best_model)\n","                  optimizer.load_state_dict(best_optimizer)\n","                  break\n","\n","          val_loss /= len(val_loader)\n","\n","      model = best_model.to(device)"],"metadata":{"id":"Pk8otIfakZ5o","executionInfo":{"status":"ok","timestamp":1706565854514,"user_tz":-60,"elapsed":324,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_model(model, epochs=hparams[\"epochs\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":810},"id":"WnBAmCp38_qd","executionInfo":{"status":"error","timestamp":1706565858685,"user_tz":-60,"elapsed":2882,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"9812749a-f59a-433e-e8be-6d797d8e86e3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["\rTraining Epoch [1/10]:   0%|                                                                                                    | 0/2 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","About to perform training iteration\n","[...] Training Step\n","[...] Loading data\n","[...] Making predictions\n","\t[...] conv_block\n","\t[...] reshape\n","\t[...] recurrent_block\n","\t[...] output_block\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Epoch [1/10]:   0%|                                                                                                    | 0/2 [00:02<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Predictions shape: torch.Size([3, 99, 1782])\n","\n","Predictions shape: torch.Size([99, 3, 1782])\n","\n","About to calculate input lengths\n","Input lengths calculated: None\n","\n","About to calculate target lengths\n","Target lengths calculated: tensor([21, 32, 25])\n","\n","About to calculate loss\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"TypeError","evalue":"ctc_loss() received an invalid combination of arguments - got (Tensor, Tensor, NoneType, Tensor, int, int, bool), but expected one of:\n * (Tensor log_probs, Tensor targets, tuple of ints input_lengths, tuple of ints target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, Tensor, !NoneType!, !Tensor!, int, int, bool)\n * (Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, Tensor, !NoneType!, Tensor, int, int, bool)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d8a521bc08fb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-578f4a7245a4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_func, epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m               \u001b[0;31m# perform training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAbout to perform training iteration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training iteration completed! Loss: loss.item()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m               \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/7-programming/music_recognition/networks/monophonic_nn.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, loss_func, device)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAbout to calculate loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss calculated: {loss.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# obtain weight updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1770\u001b[0;31m         return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n\u001b[0m\u001b[1;32m   1771\u001b[0m                           self.zero_infinity)\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2654\u001b[0m             \u001b[0mblank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m         )\n\u001b[0;32m-> 2656\u001b[0;31m     return torch.ctc_loss(\n\u001b[0m\u001b[1;32m   2657\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m     )\n","\u001b[0;31mTypeError\u001b[0m: ctc_loss() received an invalid combination of arguments - got (Tensor, Tensor, NoneType, Tensor, int, int, bool), but expected one of:\n * (Tensor log_probs, Tensor targets, tuple of ints input_lengths, tuple of ints target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, Tensor, !NoneType!, !Tensor!, int, int, bool)\n * (Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, Tensor, !NoneType!, Tensor, int, int, bool)\n"]}]},{"cell_type":"markdown","source":["## Check Model Performance"],"metadata":{"id":"T-BS6rQahfFk"}},{"cell_type":"code","source":[],"metadata":{"id":"iQznrmoKhjrS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save Model"],"metadata":{"id":"1YxyXfUiyO-N"}},{"cell_type":"code","source":[],"metadata":{"id":"4uyvlN5CyQct"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"i2dl","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}