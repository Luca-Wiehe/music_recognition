{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8lXIGhqTOdV","executionInfo":{"status":"ok","timestamp":1705446241202,"user_tz":-60,"elapsed":3666,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"ee80beef-e3f0-401d-ce80-d930512a52a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","['.git', '.gitignore', 'README.md', 'data', 'data_loader.ipynb']\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# set path to project folder\n","gdrive_path='/content/gdrive/MyDrive/7-programming/music_recognition/'\n","\n","# mount Google Drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# navigate to Google Drive folder\n","os.chdir(gdrive_path)\n","\n","# check that we are in the right folder\n","print(sorted(os.listdir()))"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"WnaZh10lTOdZ","executionInfo":{"status":"ok","timestamp":1705449424900,"user_tz":-60,"elapsed":266,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"outputs":[],"source":["import torch\n","import torch.utils.data as data\n","from torchvision import transforms\n","from PIL import Image\n","\n","class PrimusDataset(data.Dataset):\n","    def __init__(self, data_path, transform=None):\n","        self.data_path = data_path\n","        self.transform = transform\n","        self.data = []\n","\n","        # iterate through each subdirectory (corresponding to a sample)\n","        for sample_dir in os.listdir(data_path):\n","\n","            sample_dir_path = os.path.join(data_path, sample_dir)\n","\n","            image_file = None\n","            semantic_file = None\n","\n","            # .png-file contains image, .semantic-file contains labels\n","            for file in os.listdir(sample_dir_path):\n","                if file.endswith(\".png\"):\n","                    image_file = os.path.join(sample_dir_path, file)\n","                elif file.endswith(\".semantic\"):\n","                    semantic_file = os.path.join(sample_dir_path, file)\n","\n","            # check if a (image, label)-pair could be found\n","            if image_file and semantic_file:\n","                self.data.append((image_file, semantic_file))\n","            else:\n","                print(f\"Couldn't find {'Image in ' + str(sample_dir_path) if not image_file else 'Labels in ' + str(sample_dir_path)}!\")\n","\n","\n","    def __getitem__(self, index):\n","        # function to transform image to tensor\n","        to_tensor = transforms.ToTensor()\n","\n","        # obtain path for image and label at given index\n","        image_path, labels_path = self.data[index]\n","\n","        # read image and label\n","        image = Image.open(image_path).convert('L')\n","        image = to_tensor(image)\n","        with open(labels_path, 'r') as file:\n","            labels = file.read()\n","\n","        # apply transforms to image if specified\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, labels\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"NrqhqMLoTOda","executionInfo":{"status":"ok","timestamp":1705449426224,"user_tz":-60,"elapsed":2,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","from prettytable import PrettyTable\n","\n","def split_data(dataset, ratio=(0.6, 0.2, 0.2)):\n","    \"\"\"\n","    Applies train-validation-test split to a given dataset\n","\n","    Parameters:\n","        dataset: Dataset to which split will be applied\n","        ratio: Ratio represented as tuple of shape (train, val, test)\n","\n","    Returns:\n","        train_data: Dataset for Training\n","            (i.e. fitting models)\n","        val_data: Dataset for Validation\n","            (i.e. evaluating performance of models to tune hyperparameters)\n","        test_data: Dataset for Testing\n","            (i.e. evaluating final performance)\n","    \"\"\"\n","    # calculate sizes of dataset\n","    train_size, val_size = int(ratio[0] * len(dataset)), int(ratio[1] * len(dataset))\n","    test_size = len(dataset) - train_size - val_size\n","\n","    # apply split\n","    train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n","\n","    # print table of dataset sizes\n","    table = PrettyTable()\n","    table.field_names = [\"Dataset\", \"# Samples\"]\n","    table.align[\"Dataset\"] = \"l\"\n","    table.add_row([\"Train\", len(train_data)])\n","    table.add_row([\"Validation\", len(val_data)])\n","    table.add_row([\"Test\", len(test_data)])\n","\n","    print(table)\n","\n","    return train_data, val_data, test_data"]},{"cell_type":"code","source":["# load dataset\n","data_root = os.path.join(gdrive_path, 'data')\n","dataset = PrimusDataset(data_path=data_root)\n","\n","# apply train-val-test split\n","train_data, val_data, test_data = split_data(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wo3o5HHcTdwY","executionInfo":{"status":"ok","timestamp":1705449427878,"user_tz":-60,"elapsed":20,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"e17c688a-b2e9-475c-aaa0-323ff4a70334"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+-----------+\n","| Dataset    | # Samples |\n","+------------+-----------+\n","| Train      |     6     |\n","| Validation |     2     |\n","| Test       |     2     |\n","+------------+-----------+\n"]}]},{"cell_type":"code","source":["train_data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EdxMv8OCUB2I","executionInfo":{"status":"ok","timestamp":1705449432304,"user_tz":-60,"elapsed":798,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"852825aa-b762-437a-ed96-c0f6e8be39b1"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          ...,\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n"," 'clef-C1\\tkeySignature-EbM\\ttimeSignature-C\\tmultirest-4\\tbarline\\trest-half\\tnote-G4_eighth\\tnote-G4_sixteenth\\tnote-G4_sixteenth\\tnote-G4_eighth\\tnote-A4_eighth\\tbarline\\tnote-B4_eighth\\tnote-Bb4_eighth\\trest-eighth\\tnote-Bb4_eighth\\tnote-Bb4_eighth\\tnote-Bb4_eighth\\tnote-A4_eighth\\tnote-Bb4_eighth\\tbarline\\tnote-G4_quarter\\t')"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":[],"metadata":{"id":"PO7qUjmahNry"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"i2dl","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}