{"cells":[{"cell_type":"markdown","metadata":{"id":"MI67emSTOyRm"},"source":["## Connect to Google Colab"]},{"cell_type":"markdown","metadata":{"id":"wbrUMjD_O2b_"},"source":["To train the network on a GPU, Google Colab will be used. To connect to Google Colab successfully, adjust `gdrive_path` so that it represents the root folder of the repository. In case you don't want to use Google Colab you can skip this cell."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3655,"status":"ok","timestamp":1706606996227,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"},"user_tz":-60},"id":"c8lXIGhqTOdV","outputId":"b18d3081-bfef-45d6-b465-09c41c2d5f1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","['.git', '.gitignore', 'README.md', 'data', 'data_loader.ipynb', 'networks', 'utils']\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# set path to project folder\n","gdrive_path='/content/gdrive/MyDrive/7-programming/music_recognition/'\n","\n","# mount Google Drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# navigate to Google Drive folder\n","os.chdir(gdrive_path)\n","\n","# check that we are in the right folder\n","print(sorted(os.listdir()))"]},{"cell_type":"markdown","metadata":{"id":"3wqFniGfPziU"},"source":["## Manage Imports"]},{"cell_type":"markdown","metadata":{"id":"IqY3wxHvP7Ra"},"source":["Throughout the repository, we will use a wide range of external packages. If you use Google Colab, these packages need to be installed using `pip` every time you restart the kernel. If you don't use Google Colab, you can create a virtual environment to manage packages and versions. In this case, you only need to install these packages with `pip` once."]},{"cell_type":"code","source":["!python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n","!python -m pip install tensorboard==2.8.0\n","!python -m pip install pytorch-lightning==1.6.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6CiTZttcg9l","executionInfo":{"status":"ok","timestamp":1706607025938,"user_tz":-60,"elapsed":25934,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"8d7f7e82-b988-44e3-a60c-07117320af48"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch==1.11.0+cu113 in /usr/local/lib/python3.10/dist-packages (1.11.0+cu113)\n","Requirement already satisfied: torchvision==0.12.0+cu113 in /usr/local/lib/python3.10/dist-packages (0.12.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu113) (4.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (9.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2023.11.17)\n","Requirement already satisfied: tensorboard==2.8.0 in /usr/local/lib/python3.10/dist-packages (2.8.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.60.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (3.5.2)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.23.5)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (3.0.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (0.42.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard==2.8.0) (2.1.4)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.8.0) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0) (3.2.2)\n","Requirement already satisfied: pytorch-lightning==1.6.0 in /usr/local/lib/python3.10/dist-packages (1.6.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (1.23.5)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (1.11.0+cu113)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (4.66.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (6.0.1)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (2023.6.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (2.8.0)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (1.3.0.post0)\n","Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (0.3.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (23.2)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (3.9.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.60.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (3.5.2)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (3.0.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.42.0)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.4.1->pytorch-lightning==1.6.0) (0.10.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (4.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (2.1.4)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (3.2.2)\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":97,"status":"ok","timestamp":1706605360312,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"},"user_tz":-60},"id":"0325N3hoQldM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4225b4ea-5aaf-404e-cfae-ba24fbce162d"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["import torch\n","from data.primus_dataset import PrimusDataset, split_data, visualize_sample\n","\n","# set up default cuda device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"Xc7txo1hR6jd"},"source":["## Load Data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dccWQ1soSGfW"},"source":["Next, we will load the Primus dataset that will be used throughout this notebook. The dataset was published as part of the following publication:\n","\n","```\n","@inproceedings{calvo2018camera,\n","  title={Camera-PrIMuS: Neural End-to-End Optical Music Recognition on Realistic Monophonic Scores.},\n","  author={Calvo-Zaragoza, Jorge and Rizo, David},\n","  booktitle={ISMIR},\n","  pages={248--255},\n","  year={2018}\n","}\n","```\n","\n","It contains 87,678 labeled images of monophonic scores and can be downloaded from https://grfia.dlsi.ua.es/primus/. Make sure to specify the `data_root`-path to the dataset correctly. In our case, we stored it in `/<repository_root>/data/primus/`. Each sample in that dataset consists of a directory that is named e.g. `000051650-1_1_1` and contains 6 files.\n","*   `000051650-1_1_1_distorted.jpg`: The sample image with applied transformations\n","*   `000051650-1_1_1.agnostic`: Graphical elements with their position in the score\n","*   `000051650-1_1_1.mei`: Semantically labeled image in `.mei`-format, i.e. XML representation\n","*   `000051650-1_1_1.png`: The original image without any transformations\n","*   `000051650-1_1_1.semantic`: Semantically labeled image\n","*   `regular_pae.pae`: Image in format of data source taken from RISM-dataset\n","\n","Note that we will make predictions based on the undistorted `.png`-image and the `.semantic`-labeling of music notation.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":293,"status":"ok","timestamp":1706605366678,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"},"user_tz":-60},"id":"Wo3o5HHcTdwY","outputId":"30700240-25e7-4a3d-c95f-e57af5b92fc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+-----------+\n","| Dataset    | # Samples |\n","+------------+-----------+\n","| Train      |     6     |\n","| Validation |     2     |\n","| Test       |     2     |\n","+------------+-----------+\n"]}],"source":["# load dataset\n","data_path = os.path.join(gdrive_path, 'data', 'primus')\n","vocabulary_path = os.path.join(gdrive_path, 'data', 'semantic_labels.txt')\n","dataset = PrimusDataset(data_path=data_path, vocabulary_path=vocabulary_path)\n","\n","# apply train-val-test split\n","train_data, val_data, test_data = split_data(dataset)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"elapsed":1155,"status":"ok","timestamp":1706605369826,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"},"user_tz":-60},"id":"EdxMv8OCUB2I","outputId":"cef501ce-9f2b-48af-dc8a-9aab4089d5b6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["labels.shape: torch.Size([23])\n"]}],"source":["visualize_sample(train_data)"]},{"cell_type":"markdown","source":["## Train Model"],"metadata":{"id":"iR5oynqTd6Km"}},{"cell_type":"markdown","source":["Having loaded the dataset, we can start defining our model. For this, we need to define some hyperparameters that will be necessary for training. Use the following cell to define your own training parameters."],"metadata":{"id":"Gic20FI8fI3A"}},{"cell_type":"code","source":["hparams = {\n","    \"batch_size\": 3,\n","    \"epochs\": 10,\n","    \"early_stop\": {\n","        \"patience\": 25\n","    },\n","    \"scheduler\": {\n","        \"plateau_patience\": 10,\n","        \"plateau_decay\": 0.4,\n","        \"threshold\": 0.00005,\n","        \"threshold_mode\": \"rel\",\n","        \"mode\": \"min\",\n","        \"cooldown\": 0,\n","        \"eps\": 1e-8\n","    },\n","    \"optimizer\": {\n","        \"learning_rate\": 1e-3,\n","        \"weight_decay\": 1e-4\n","    },\n","}"],"metadata":{"id":"i5ZHUQKfddjS","executionInfo":{"status":"ok","timestamp":1706605373546,"user_tz":-60,"elapsed":328,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["Given these model parameters, we can initialize our model. For the Primus dataset we use the `MonophonicModel` that is defined in `monophonic_nn.py`. It is suitable to predict sequences of unknown length (since we don't know in advance how many music symbols are depicted in an image). Note that we primarily focus on monophonic scores, i.e. a single line of music notation."],"metadata":{"id":"FNzgox-we5q9"}},{"cell_type":"code","source":["from networks.monophonic_nn import MonophonicModel\n","from utils.utils import create_tqdm_bar\n","\n","model = MonophonicModel(\n","    hparams=hparams,\n","    output_size=len(dataset.index_to_vocabulary)\n",")"],"metadata":{"id":"Et2PrR-FdzLw","executionInfo":{"status":"ok","timestamp":1706605439433,"user_tz":-60,"elapsed":192,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["Next, we run the training epochs of our model. After each epoch, we run our model on validation data to track the model's performance over time. We use early stopping which means that we terminate training early once the performance gain doesn't improve for several epochs."],"metadata":{"id":"1B4ilDCQiRBe"}},{"cell_type":"code","source":["def collate_fn(batch):\n","    \"\"\" Custom collate function for DataLoader. \"\"\"\n","    # Separate data and labels\n","    data, labels = zip(*batch)\n","\n","    # Find the maximum width and height in the batch\n","    max_width = max([d.shape[2] for d in data])\n","    max_height = max([d.shape[1] for d in data])\n","\n","    # Handling data (images)\n","    padded_data = []\n","    for d in data:\n","        # Calculate padding size\n","        padding_left = (max_width - d.shape[2]) // 2\n","        padding_right = max_width - d.shape[2] - padding_left\n","        padding_top = (max_height - d.shape[1]) // 2\n","        padding_bottom = max_height - d.shape[1] - padding_top\n","\n","        # Apply padding\n","        padded = torch.nn.functional.pad(d, (padding_left, padding_right, padding_top, padding_bottom), \"constant\", 0)\n","        padded_data.append(padded)\n","\n","    # Stack all the padded images and labels into tensors\n","    padded_data = torch.stack(padded_data)\n","\n","    # Handling labels\n","    # Find the maximum label length\n","    max_label_len = max([len(l) for l in labels])\n","\n","    # Pad labels\n","    padded_labels = []\n","    for l in labels:\n","        # Padding length\n","        padding_len = max_label_len - len(l)\n","\n","        # Pad and append\n","        padded_label = torch.cat((l, torch.full((padding_len,), -1, dtype=torch.long))) # Using -1 as padding token\n","        padded_labels.append(padded_label)\n","\n","    # Stack padded labels\n","    labels = torch.stack(padded_labels)\n","\n","    return padded_data, labels"],"metadata":{"id":"ZFi5hrjoJyjj","executionInfo":{"status":"ok","timestamp":1706605443507,"user_tz":-60,"elapsed":509,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def train_model(model, loss_func=torch.nn.CTCLoss(blank=0), epochs=20):\n","\n","      # obtain model optimizer\n","      optimizer = model.optimizer\n","\n","      # decrease lr of optimizer when reaching a plateau\n","      scheduler_hparams = hparams[\"scheduler\"]\n","      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","                  optimizer=optimizer,\n","                  mode=scheduler_hparams[\"mode\"],\n","                  patience=scheduler_hparams[\"plateau_patience\"],\n","                  factor=scheduler_hparams[\"plateau_decay\"],\n","                  threshold=scheduler_hparams[\"threshold\"],\n","                  threshold_mode=scheduler_hparams[\"threshold_mode\"],\n","                  cooldown=scheduler_hparams[\"cooldown\"],\n","                  eps=scheduler_hparams[\"eps\"],\n","      )\n","\n","      # select device for model\n","      model = model.to(device)\n","\n","      # initialize early stopping criteria\n","      stopping_hparams = hparams[\"early_stop\"]\n","      best_loss, best_model, best_optimizer = -1, None, None\n","      patience, current_patience = stopping_hparams[\"patience\"], stopping_hparams[\"patience\"]\n","\n","      # run epochs\n","      for epoch in range(epochs):\n","\n","          # training for each minibatch\n","          train_loader = torch.utils.data.DataLoader(train_data, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n","          train_loop = create_tqdm_bar(train_loader, desc=f\"Training Epoch [{epoch + 1}/{epochs}]\")\n","          train_loss, val_loss = 0, 0\n","\n","          for train_iteration, batch in train_loop:\n","\n","              # perform training step\n","              print(\"\\nAbout to perform training iteration\")\n","              loss = model.training_step(batch, loss_func, device)\n","              print(f\"Training iteration completed! Loss: loss.item()\")\n","              train_loss += loss.item()\n","\n","              # Update the progress bar.\n","              train_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(\n","                  train_loss / (train_iteration + 1)), val_loss = \"{:.8f}\".format(val_loss)\n","              )\n","\n","          # validation for each minibatch\n","          val_loader = torch.utils.data.DataLoader(val_data, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n","          val_loop = create_tqdm_bar(val_loader, f\"Validation Epoch [{epoch + 1}/{epochs}]\")\n","          val_loss = 0\n","\n","          for val_iteration, batch in val_loop:\n","\n","              # perform validation step\n","              loss = model.validation_step(batch, loss_func, device)\n","              val_loss += loss.item()\n","\n","              # update the progress bar.\n","              val_loop.set_postfix(val_loss = \"{:.8f}\".format(val_loss / (val_iteration + 1)))\n","\n","          # learning rate update for each epoch\n","          pre_lr = optimizer.param_groups[0][\"lr\"]\n","          scheduler.step(val_loss)\n","          post_lr = optimizer.param_groups[0]['lr']\n","          if post_lr < pre_lr:\n","            print(\"Loading best model due to learning rate decrease.\")\n","            model = best_model.to(device)\n","\n","          # check for early stopping\n","          if val_loss < best_loss or best_loss == -1:\n","              current_patience = patience\n","              best_loss = val_loss\n","              best_model = model\n","              best_optimizer = optimizer.state_dict()\n","          else:\n","              current_patience -= 1\n","\n","              if current_patience == 0:\n","                  print(f\"\\n{'===' * 10}\\nStopping early at epoch {epoch}\\n{'===' * 10}\")\n","                  model.load_state_dict(best_model)\n","                  optimizer.load_state_dict(best_optimizer)\n","                  break\n","\n","          val_loss /= len(val_loader)\n","\n","      model = best_model.to(device)"],"metadata":{"id":"Pk8otIfakZ5o","executionInfo":{"status":"ok","timestamp":1706605449571,"user_tz":-60,"elapsed":336,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["train_model(model, epochs=hparams[\"epochs\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":893},"id":"WnBAmCp38_qd","executionInfo":{"status":"error","timestamp":1706605461166,"user_tz":-60,"elapsed":6947,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"758e06ff-9d93-450e-b9ca-e2dae709be13"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["\rTraining Epoch [1/10]:   0%|                                                                                                    | 0/2 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","About to perform training iteration\n","[...] Training Step\n","[...] Loading data\n","[...] Making predictions\n","\t[...] conv_block\n","\t[...] reshape\n","\t[...] recurrent_block (shape: torch.Size([3, 101, 2304]))\n","\t[...] output_block\n","[...] Calculating loss\n","[...] Backpropagating with loss 21.024072647094727\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch [1/10]:  50%|█████████████████████                     | 1/2 [00:03<00:03,  3.38s/it, curr_train_loss=21.02407265, val_loss=0.00000000]"]},{"output_type":"stream","name":"stdout","text":["[...] Updating weights\n","\n","[+] Training Step finished\n","Training iteration completed! Loss: loss.item()\n","\n","About to perform training iteration\n","[...] Training Step\n","[...] Loading data\n","[...] Making predictions\n","\t[...] conv_block\n","\t[...] reshape\n","\t[...] recurrent_block (shape: torch.Size([3, 83, 2304]))\n","\t[...] output_block\n","[...] Calculating loss\n","[...] Backpropagating with loss 21.898527145385742\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch [1/10]: 100%|██████████████████████████████████████████| 2/2 [00:06<00:00,  3.05s/it, curr_train_loss=21.46129990, val_loss=0.00000000]\n"]},{"output_type":"stream","name":"stdout","text":["[...] Updating weights\n","\n","[+] Training Step finished\n","Training iteration completed! Loss: loss.item()\n"]},{"output_type":"stream","name":"stderr","text":["Validation Epoch [1/10]:   0%|                                                                                                  | 0/1 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"TypeError","evalue":"MonophonicModel.validation_step() takes 0 positional arguments but 4 were given","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-d8a521bc08fb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-f7b145e252c2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_func, epochs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m               \u001b[0;31m# perform validation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m               \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: MonophonicModel.validation_step() takes 0 positional arguments but 4 were given"]}]},{"cell_type":"markdown","source":["## Check Model Performance"],"metadata":{"id":"T-BS6rQahfFk"}},{"cell_type":"code","source":[],"metadata":{"id":"iQznrmoKhjrS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save Model"],"metadata":{"id":"1YxyXfUiyO-N"}},{"cell_type":"code","source":[],"metadata":{"id":"4uyvlN5CyQct"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}